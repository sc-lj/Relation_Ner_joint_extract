{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.0 64-bit ('python37': conda)",
   "metadata": {
    "interpreter": {
     "hash": "dcdaf53920453f3b150cafe1b9dfb3d5674b78b69806e86d71e7018a3889c6d8"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "files = \"./CasRel/data/baidu/duie_train.json\"\n",
    "with open(files,\"rb\") as f:\n",
    "    data = f.readlines()\n",
    "index = 0\n",
    "a = ['上映时间', '专业代码', '改编自', '修业年限', '成立日期', 'ondate_date', '董事长', '注册资本', '占地面积', '人口数量', '所属专辑', 'inwork_影视作品', '编剧', '歌手', '票房', '导演', '主演', 'period_number', '邮政编码', '作者', '简称', '面积', '作曲', '作词']\n",
    "predicates = set()\n",
    "max_leng = 0\n",
    "for line in data:\n",
    "    line = json.loads(line)\n",
    "    spo_list = line['spo_list']\n",
    "    text = line['text']\n",
    "    max_leng = max(len(text),max_leng)\n",
    "    for spo in spo_list:\n",
    "        s = spo['subject']\n",
    "        p = spo['object']['@value']\n",
    "        predicate = spo['predicate']\n",
    "        if ( s.isdigit() or p.isdigit() ) and predicate == a[1]:\n",
    "            # print(p+\"\\t\"+predicate+\"\\t\"+s+\"\\t\"+text)\n",
    "            predicates.add(predicate)\n",
    "            index += 1\n",
    "# print(predicates)\n",
    "print(max_leng)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "aa = np.array([[0.0349,  0.0670, -0.0612, 0.0280, -0.0222,  0.0422],\n",
    "         [-1.6719,  0.1242, -0.6488, 0.3313, -1.3965, -0.0682],\n",
    "         [-1.3419,  0.4485, -0.6589, 0.1420, -0.3260, -0.4795]])\n",
    "bb = np.array([[-0.0658, -0.1490, -0.1684, 0.7188,  0.3129, -0.1116],\n",
    "         [-0.2098, -0.2980,  0.1126, 0.9666, -0.0178,  0.1222],\n",
    "         [ 0.1179, -0.4622, -0.2112, 1.1151,  0.1846,  0.4283]])\n",
    "cc = np.where(aa>0)     #合并a,b两个tensor，如果a中元素大于0\n",
    "dd = np.where(bb>0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "sub_heads = torch.tensor([2,3,2,5,2,7,6,7,8,2])\n",
    "sub_tails = torch.tensor([5,7,5,2,7,3,8,7,8,7])\n",
    "sub_head_mapping = torch.zeros(10,1,9)\n",
    "sub_tail_mapping = torch.zeros(10,1,9)\n",
    "sub_head_mapping.scatter_(-1,sub_heads.reshape(10,1,1),1)\n",
    "sub_tail_mapping.scatter_(-1,sub_tails.reshape(10,1,1),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "text = \"内地影视演员龚蓓苾携手周渝民、迪丽热巴、张彬彬、刘芮麟、赖艺、代斯、张赫等主创悉数出席\"\n",
    "tokenizer = BertTokenizer.from_pretrained(\"../chinese_bert_base\")\n",
    "# print(tokenizer.tokenize(text))\n",
    "# token = ['龚', '蓓', '苾', '携', '手', '周', '渝', '民', '、', '迪', '丽', '热', '巴', '、', '张', '彬', '彬', '、', '刘', '芮', '麟', '、', '赖', '艺', '、', '代', '斯', '、', '张', '赫', '等', '主', '创', '悉', '数', '出', '席']\n",
    "# tokenizer.convert_tokens_to_ids(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 30, 100])\ntorch.Size([10, 30, 4, 100])\ntorch.Size([10, 30, 4, 50])\ntorch.Size([10, 30, 4, 25])\ntorch.Size([10, 30, 4, 25, 2])\ntorch.Size([10, 30, 4, 50])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "head_size = 50\n",
    "heads=4\n",
    "a = torch.rand(10,30,head_size* heads * 2)\n",
    "input1 = torch.split(a,head_size*2,dim = -1)\n",
    "print(input1[0].shape)\n",
    "input2 = torch.stack(input1, axis=-2)\n",
    "print(input2.shape)\n",
    "qw, kw = input2[..., :head_size], input2[..., head_size:]\n",
    "print(qw.shape)\n",
    "print(qw[..., 1::2].shape)\n",
    "qw2 = torch.stack([-qw[..., 1::2], qw[..., ::2]], 4)\n",
    "print(qw2.shape)\n",
    "qw2 = torch.reshape(qw2, qw.shape)\n",
    "print(qw2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(10, 20, 400)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "inputs = tf.random_normal((10,20,head_size* heads * 2))\n",
    "print(inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4\n(10, 20, 100)\n(10, 20, 4, 100)\n(10, 20, 4, 50)\n(10, 20, 4, 25, 2)\n(10, 20, 4, 50)\n"
     ]
    }
   ],
   "source": [
    "input1 = tf.split(inputs, heads, axis=-1)\n",
    "print(len(input1))\n",
    "print(input1[0].shape)\n",
    "input2 = K.stack(input1, axis=-2)\n",
    "print(input2.shape)\n",
    "qw, kw = input2[..., :head_size], input2[..., head_size:]\n",
    "print(qw.shape)\n",
    "qw2 = K.stack([-qw[..., 1::2], qw[..., ::2]], 4)\n",
    "print(qw2.shape)\n",
    "qw2 = K.reshape(qw2, K.shape(qw))\n",
    "print(qw2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 100, 25, 2])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.rand((10,100,20))\n",
    "output_dim = 50\n",
    "position_ids = torch.arange(0, 100, dtype=torch.float).reshape(1,-1)\n",
    "indices = torch.arange(0, output_dim // 2, dtype=torch.float)\n",
    "indices = torch.pow(10000.0, -2 * indices / output_dim).reshape(1,-1)\n",
    "embeddings = torch.matmul(position_ids.unsqueeze(-1),indices.unsqueeze(0))\n",
    "embeddings = torch.stack([torch.sin(embeddings), torch.cos(embeddings)], axis=-1)\n",
    "# embeddings = torch.reshape(embeddings, (-1, 100, output_dim))\n",
    "print(embeddings.shape)\n",
    "# embeddings = embeddings.repeat([10, 1, 1])\n",
    "# torch.cat([inputs, embeddings],axis=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(25,)\n(1, 100, 50)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor 'concat:0' shape=(10, 100, 70) dtype=float32>"
      ]
     },
     "metadata": {},
     "execution_count": 76
    }
   ],
   "source": [
    "inputs = tf.random_normal((10,100,20))\n",
    "position_ids = K.arange(0, 100, dtype=K.floatx())[None]\n",
    "\n",
    "indices = K.arange(0, output_dim // 2, dtype=K.floatx())\n",
    "indices = K.pow(10000.0, -2 * indices / output_dim)\n",
    "print(indices.shape)\n",
    "embeddings = tf.einsum('bn,d->bnd', position_ids, indices)\n",
    "embeddings = K.stack([K.sin(embeddings), K.cos(embeddings)], axis=-1)\n",
    "embeddings = K.reshape(embeddings, (-1, 100, output_dim))\n",
    "\n",
    "print(embeddings.shape)\n",
    "embeddings = K.tile(embeddings, [10, 1, 1])\n",
    "K.concatenate([inputs, embeddings])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 2, 30, 20])\ntorch.Size([10, 2, 30, 20])\ntorch.Size([10, 2, 20, 30])\ntorch.Size([10, 2, 20, 30])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[[True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          ...,\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "         [[True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          ...,\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True]]],\n",
       "\n",
       "\n",
       "        [[[True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          ...,\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "         [[True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          ...,\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True]]],\n",
       "\n",
       "\n",
       "        [[[True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          ...,\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "         [[True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          ...,\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          ...,\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "         [[True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          ...,\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True]]],\n",
       "\n",
       "\n",
       "        [[[True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          ...,\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "         [[True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          ...,\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True]]],\n",
       "\n",
       "\n",
       "        [[[True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          ...,\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "         [[True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          ...,\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True]]]])"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "qw = torch.rand(10,30,2,20)\n",
    "kw = torch.rand(10,30,2,20)\n",
    "# torch.matmul(qw.transpose(2,1),kw.permute(0,3,1,2))\n",
    "print(qw.transpose(2,1).shape)\n",
    "kww = kw.transpose(2,1)\n",
    "print(kww.shape)\n",
    "print(kww.transpose(3,2).shape)\n",
    "print(kw.permute(0,2,3,1).shape)\n",
    "kww.transpose(3,2) == kw.permute(0,2,3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3.1007533\n[0.6931472 1.6931472 2.6931472]\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[0., 1., 2.], [0., 1., 2.]])\n",
    "y1 = tf.reduce_logsumexp(x)             # 3.1007533\n",
    "y2 = tf.reduce_logsumexp(x, 0)\n",
    "\n",
    "sess = tf.Session()\n",
    "# Evaluate the tensor `c`.\n",
    "\n",
    "print(sess.run(y1))\n",
    "print(sess.run(y2))\n",
    "x = torch.tensor([[0., 1., 2.], [0., 1., 2.]])\n",
    "y2 = torch.logsumexp(x, dim=0)\n",
    "print(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(0.9429)\ntensor(0.5409)\ntensor(0.8270)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.parameter import Parameter, UninitializedParameter\n",
    "a= torch.zeros((3,20))\n",
    "b= torch.zeros((3,20))\n",
    "a[0,5] = 1\n",
    "a[1,7] = 1\n",
    "a[2,2] = 1\n",
    "b[0,10] = 1\n",
    "b[1,16]=1\n",
    "b[2,14]=1\n",
    "c = torch.rand(3,20,20)\n",
    "print(c[0,5,10])\n",
    "print(c[1,7,16])\n",
    "print(c[2,2,14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 0,  5, 10],\n",
       "        [ 1,  7, 16],\n",
       "        [ 2,  2, 14]])"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "def th_gather_nd(x, coords):\n",
    "    x = x.contiguous()\n",
    "\n",
    "    inds = coords.mv(torch.LongTensor(x.stride()))\n",
    "    x_gather = torch.index_select(x.contiguous().view(-1), 0, inds)\n",
    "    return x_gather\n",
    "head = torch.nonzero(a)[:,1]\n",
    "tail = torch.nonzero(b)[:,1]\n",
    "head_tail = torch.vstack((head,tail)).T\n",
    "batch = c.shape[0]\n",
    "index = torch.hstack([torch.arange(batch).reshape(-1,1),head_tail])\n",
    "# torch.index_select(c,[1,2],head_tail)\n",
    "# th_gather_nd(c,index)\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0.9429, 0.5409, 0.8270])"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "idx_chunked = head_tail.chunk(2,1)\n",
    "masked = c[torch.arange(3).view(3,1),idx_chunked[0].squeeze(),idx_chunked[1].squeeze()]\n",
    "# masked.expand(1,*masked.shape)\n",
    "diag = torch.diag(masked)\n",
    "diag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[5],\n",
       "         [7]]),\n",
       " tensor([[10],\n",
       "         [16]]))"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "idx_chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[[1],\n",
       "          [0],\n",
       "          [1],\n",
       "          [0]],\n",
       " \n",
       "         [[1],\n",
       "          [1],\n",
       "          [1],\n",
       "          [1]]]),\n",
       " tensor([[[0],\n",
       "          [0],\n",
       "          [1],\n",
       "          [0]],\n",
       " \n",
       "         [[0],\n",
       "          [1],\n",
       "          [1],\n",
       "          [0]]]))"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "torch.randint(0,2,(2,4,2)).chunk(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = index[:,0]\n",
    "start = index[:,1]\n",
    "end = index[:,2]\n",
    "d= a+b\n",
    "seq_len = d.shape[-1]\n",
    "seqs = torch.zeros_like(d)\n",
    "seqs[:,:]= torch.arange(seq_len)\n",
    "# d[:,:start] = 1\n",
    "start_index = (seqs<start.reshape(-1,1)).int()\n",
    "end_index = (seqs<=end.reshape(-1,1)).int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]],\n",
       "       dtype=torch.int32)"
      ]
     },
     "metadata": {},
     "execution_count": 134
    }
   ],
   "source": [
    "new_index = start_index+end_index\n",
    "new_range = (new_index==1).int()\n",
    "new_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]],\n",
       "       dtype=torch.int32)"
      ]
     },
     "metadata": {},
     "execution_count": 135
    }
   ],
   "source": [
    "    def get_range_head_tail(head,tail):\n",
    "        head_index = torch.nonzero(head)[:,1]\n",
    "        tail_index = torch.nonzero(tail)[:,1]\n",
    "        mask = torch.zeros_like(head)\n",
    "        seq_len = head.shape[-1]\n",
    "        mask[:,:]= torch.arange(seq_len)\n",
    "        start_range = (mask<head_index.reshape(-1,1)).int()\n",
    "        end_range = (mask<=tail_index.reshape(-1,1)).int()\n",
    "        new_mask = start_range+end_range\n",
    "        new_mask = (new_mask==1).int()\n",
    "        return new_mask\n",
    "get_range_head_tail(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000]])"
      ]
     },
     "metadata": {},
     "execution_count": 152
    }
   ],
   "source": [
    "a= torch.rand(10,1,100)\n",
    "(a/a.sum(-1,keepdim=True)).sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}